"""
Calculon is a analytical model for estimating LLM training times for given architectures
on particular hardware. It is described in the paper:

    Isaev, Mikhail, et al. "Calculon: a methodology and tool for high-level co-design of 
    systems and large language models." SC23 Proceedings
    https://dl.acm.org/doi/pdf/10.1145/3581784.3607102

The code is available at https://github.com/calculon-ai/calculon
which this module assumes is already cloned into the third_party directory.

Calculon requires installing `psutil`, which can be pip installed via:

    pip install psutil

Since Calculon by default supports A100 GPUs, we are able to use the default files that
are already setup in Calculon, and therefore have added two systems which have A100 GPUs:
Selene and Perlmutter. Example run commands:

    python main.py run --system selene -w calculon
    python main.py run --system perlmutter -w calculon

This code is currently setup to generate synthetic traces for four different LLM models:
megatron-22B, gpt3-175B, turing-530B, and megatron-1T. These four tests can take a couple
**hours** to run. On first run, consider commenting out the last three models to only test
the smallest case, megatron-22B. The parameter `llm_models_tests` below defines which tests
are run.

Finally, the code below is setup to uses previously cached results, so once the json 
files are generated by Calculon, they can be rerun very quickly again and again. 
The caveat to this is if you want to change some Calculon configurations, 
you will need to delete the cached json files in the calculon/optimal_executions folder,
to force it to regenerate new files.

"""
import math
import json
import os
import random
import subprocess
from pathlib import Path

import numpy as np

from raps.job import Job, job_dict

from .constants import ACCT_NAMES


class Calculon:
    """Calculon workload mixin for Workload class."""

    def __init__(self, *args, **kwargs):
        # NOTE: mixins usually accept (sim_config_args, system_config_dict) through Workload
        super().__init__(*args, **kwargs)

    def calculon(self, **kwargs):
        """Generate workload using Calculon backend + job trace synthesis."""
        jobs = []

        llm_models_test = [
            ["megatron-22B", 8, 4],
            ["gpt3-175B", 64, 64],
            ["turing-530B", 280, 280],
            ["megatron-1T", 512, 512],
        ]

        for llm_model, num_nodes, max_batch_size in llm_models_test:
            for partition in self.partitions:
                config = self.config_map[partition]
                gpu_system = "a100_80g"
                data_type = "float16"
                output = f"{llm_model}_{gpu_system}_{max_batch_size}_{data_type}_{num_nodes}.json"

                # call Calculon binary/subprocess to get MFU + batch time
                mfu, total_batch_time = self._run_calculon(
                    llm_model, gpu_system, max_batch_size, num_nodes, data_type, output
                )

                # derive job stats
                num_iters = 1000000  # realistic number is probably in the millions
                trace_quanta = config["TRACE_QUANTA"]

                job_time = total_batch_time * num_iters
                num_samples = math.ceil(job_time / trace_quanta) + 1
                end_time = num_samples * trace_quanta   # align job to tick grid

                # use random CPU utilizations for now
                cpu_util = random.random() * config["CPUS_PER_NODE"]
                cpu_trace = np.full(num_samples, cpu_util)  # same length
                gpu_trace = np.full(num_samples, mfu)   # length matches simulation steps

                net_tx, net_rx = [], []
                num_nodes = num_nodes // config["GPUS_PER_NODE"]

                epochs = 1
                wall_time = job_time
                for i in range(epochs):
                    job_info = job_dict(
                        nodes_required=num_nodes,
                        scheduled_nodes=[],
                        name=f"{llm_model} training for {num_iters} iterations",
                        account=ACCT_NAMES[0],
                        cpu_trace=cpu_trace,
                        gpu_trace=gpu_trace,
                        ntx_trace=net_tx,
                        nrx_trace=net_rx,
                        end_state="COMPLETED",
                        id=None,
                        priority=100,
                        partition=partition,
                        time_limit=job_time + 1,
                        start_time=0,
                        end_time=end_time,
                        expected_run_time=end_time,
                        trace_quanta=trace_quanta,
                        trace_time=job_time,
                        trace_start_time=0,
                        trace_end_time=job_time,
                    )
                    job = Job(job_info)
                    jobs.append(job)
                    wall_time += job_time

        return jobs

    def _run_calculon(self, model, system, max_batch_size, num_nodes, data_type, output):
        """Internal: run Calculon subprocess and parse result."""
        base_path = Path("third_party/calculon")
        output_dir = base_path / "optimal_executions"
        output_dir.mkdir(exist_ok=True)

        # expected files
        raw_file = output_dir / f"{output.replace('.json', '_raw.json')}"
        exec_file = output_dir / f"{output.replace('.json', '_exec.json')}"
        stats_file = output_dir / f"{output.replace('.json', '_stats.json')}"

        # if all three exist, skip running
        if raw_file.exists() and exec_file.exists() and stats_file.exists():
            print(f"[INFO] Using cached Calculon results for {output}")
            with open(raw_file) as f:
                data = json.load(f)
            first_key = list(data.keys())[0]
            stats = data[first_key]["stats"]
            mfu = stats.get("sample_rate", 0)   # or compute MFU if you want
            batch_time = stats.get("block_fw_time", 0)  # example placeholder
            return mfu, batch_time

        # otherwise, run Calculon
        opt_cmd = [
            "./bin/calculon", "llm-optimal-execution",
            f"models/{model}.json",
            str(num_nodes),
            str(max_batch_size),
            data_type,
            f"systems/{system}.json",
            str(raw_file),
        ]

        llm_cmd = [
            "./bin/calculon", "llm",
            f"models/{model}.json",
            str(exec_file),
            f"systems/{system}.json",
            str(stats_file),
        ]

        subprocess.run(opt_cmd, check=True, cwd=base_path, env={**os.environ, "PYTHONPATH": "."})
        subprocess.run(llm_cmd, check=True, cwd=base_path, env={**os.environ, "PYTHONPATH": "."})

        # parse output
        with open(raw_file) as f:
            data = json.load(f)
        first_key = list(data.keys())[0]
        stats = data[first_key]["stats"]

        mfu = stats.get("sample_rate", 0)
        batch_time = stats.get("block_fw_time", 0)

        return mfu, batch_time
